# -*- coding: utf-8 -*-
"""Untitled6.DSA210 Phase 3

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/144oFyi3avIiCetpHyztSntVoE8GOLWnE

## Importing Required Libraries

This section imports all necessary Python libraries used throughout the machine learning pipeline.

- **NumPy & Pandas** are used for numerical operations and data manipulation.
- **Scikit-learn** provides tools for preprocessing, model training, evaluation, and cross-validation.
- **Matplotlib** is used for visualizing model performance.

All libraries are standard in data science workflows and ensure reproducibility of the analysis.
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

from sklearn.model_selection import train_test_split, KFold, cross_validate
from sklearn.pipeline import Pipeline
from sklearn.compose import ColumnTransformer
from sklearn.preprocessing import StandardScaler

from sklearn.linear_model import LinearRegression
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor

from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error

"""## Data Loading and Initial Cleaning

In this step, the traditional advertising dataset and the digital marketing dataset are loaded separately.

Basic cleaning operations are applied:
- Irrelevant index columns are removed.
- Rows with missing values are dropped.
- Monetary values in the digital dataset are converted from string format (e.g., "$12,345") to numeric values.

This ensures that both datasets are analysis-ready and suitable for feature engineering and modeling.

"""

adv = pd.read_csv("advertising.csv")
dig = pd.read_csv("digital_marketing.csv")

# Advertising clean
adv = adv.drop(columns=[c for c in ["Unnamed: 0"] if c in adv.columns])
adv = adv.dropna().reset_index(drop=True)

# Digital clean
dig = dig.dropna().reset_index(drop=True)

# Acquisition_Cost: "$16,174.00" -> 16174.00
dig["Acquisition_Cost"] = (
    dig["Acquisition_Cost"].astype(str)
    .str.replace("$", "", regex=False)
    .str.replace(",", "", regex=False)
    .astype(float)
)

print("Advertising shape:", adv.shape)
print("Digital shape    :", dig.shape)
print("Advertising columns:", adv.columns.tolist())
print("Digital columns    :", dig.columns.tolist())

"""## Digital Feature Engineering

The raw digital marketing data does not directly contain all variables required for analysis.
Therefore, several meaningful features are engineered:

- **Click-Through Rate (CTR):** Measures engagement efficiency as Clicks / Impressions.
- **Conversions:** Approximated using Clicks × Conversion_Rate.
- **Digital_Spend:** Derived from acquisition cost and rescaled to thousands of dollars
  to ensure comparability with traditional advertising budgets.

These engineered features allow digital marketing performance to be analyzed alongside traditional media.

"""

# CTR
dig["CTR"] = dig["Clicks"] / dig["Impressions"].replace(0, np.nan)
dig["CTR"] = dig["CTR"].fillna(0)

# Conversions (approx.)
dig["Conversions"] = dig["Clicks"] * dig["Conversion_Rate"]

# Digital_Spend (thousands of dollars)
dig["Digital_Spend"] = dig["Acquisition_Cost"] / 1000.0

dig_ml = dig[["Digital_Spend", "CTR", "Conversions", "ROI", "Engagement_Score"]].copy()
dig_ml.describe().T

"""## Dataset Alignment and Combination

The traditional and digital datasets do not share a common campaign or time identifier.
To enable joint modeling, an index-based alignment strategy is applied:

- A subset of digital campaigns is randomly sampled to match the size of the advertising dataset.
- The two datasets are then concatenated by index.

This approach allows exploratory modeling of cross-channel effects while acknowledging that
the alignment is synthetic rather than a real-world campaign match.

"""

# Making digital side same length as advertising by sampling
n = len(adv)
dig_sample = dig_ml.sample(n=n, random_state=42).reset_index(drop=True)

# Combining by index alignment
df = pd.concat([adv.reset_index(drop=True), dig_sample], axis=1)

print("Merged df shape:", df.shape)
df.head()

"""## Cross-Channel Synergy Feature

To test whether traditional and digital channels reinforce each other,
an interaction term between TV advertising and Digital_Spend is created.

This interaction captures **non-additive effects**, where the combined influence
of TV and digital advertising may exceed their individual contributions.

The interaction term directly operationalizes the cross-channel synergy hypothesis (H3).

"""

tv_col = "TV Ad Budget ($)"
target_col = "Sales ($)"

# Safety checks
for c in [tv_col, "Radio Ad Budget ($)", "Newspaper Ad Budget ($)", target_col, "Digital_Spend", "CTR", "Conversions"]:
    if c not in df.columns:
        raise ValueError(f"Missing required column: {c}")

df["TV_x_Digital"] = df[tv_col] * df["Digital_Spend"]
df[["TV_x_Digital"]].head()

"""## Defining Features and Target Variable

In this step, the modeling variables are defined:

- **X (features):** Includes traditional media budgets, digital engagement variables,
  and the interaction term.
- **y (target):** Sales revenue.

Separating features and the target variable clarifies the prediction task
and prepares the data for machine learning models.

"""

features = [
    "TV Ad Budget ($)",
    "Radio Ad Budget ($)",
    "Newspaper Ad Budget ($)",
    "Digital_Spend",
    "CTR",
    "Conversions",
    "ROI",
    "Engagement_Score",
    "TV_x_Digital"
]
target = "Sales ($)"

X = df[features].copy()
y = df[target].copy()

"""## Train–Test Split

The dataset is split into training and testing subsets.

- **Training set:** Used to fit the models.
- **Test set:** Held out for evaluating generalization performance.

This separation ensures that model evaluation reflects performance on unseen data,
which is critical for assessing predictive validity.

"""

X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42
)

print(X_train.shape, X_test.shape)

"""## Preprocessing and Model Pipelines

Before training, numerical features are standardized using a scaler.
Standardization is important because features are measured on different scales
(e.g., budgets vs engagement rates).

Pipelines are constructed to:
- Apply preprocessing consistently
- Prevent data leakage
- Combine preprocessing and modeling into a single, reproducible workflow

Three models are prepared:
- Linear Regression
- Random Forest Regressor
- Gradient Boosting Regressor

"""

preprocess = ColumnTransformer(
    transformers=[("num", StandardScaler(), features)],
    remainder="drop"
)

lin = Pipeline(steps=[
    ("prep", preprocess),
    ("model", LinearRegression())
])

rf = Pipeline(steps=[
    ("prep", preprocess),
    ("model", RandomForestRegressor(
        n_estimators=400, random_state=42, max_depth=12, min_samples_leaf=2
    ))
])

gbr = Pipeline(steps=[
    ("prep", preprocess),
    ("model", GradientBoostingRegressor(random_state=42))
])

models = {
    "LinearRegression": lin,
    "RandomForest": rf,
    "GradBoost": gbr
}

"""## Model Training and Evaluation on Test Data

Each model is trained on the training set and evaluated on the test set.

Performance is assessed using:
- **R²:** Proportion of variance explained
- **MAE:** Average absolute prediction error
- **RMSE:** Penalizes larger errors more strongly

These metrics provide complementary perspectives on predictive accuracy and error magnitude.

"""

def reg_metrics(y_true, y_pred):
    r2 = r2_score(y_true, y_pred)
    mae = mean_absolute_error(y_true, y_pred)
    rmse = np.sqrt(mean_squared_error(y_true, y_pred))
    return r2, mae, rmse

rows = []
preds = {}

for name, model in models.items():
    model.fit(X_train, y_train)
    p = model.predict(X_test)
    preds[name] = p
    r2, mae, rmse = reg_metrics(y_test, p)
    rows.append({"Model": name, "R2": r2, "MAE": mae, "RMSE": rmse})

results = pd.DataFrame(rows).sort_values("MAE")
results

"""## Baseline Model Comparison

To verify that machine learning models provide real predictive value,
a naive baseline model is constructed.

The baseline simply predicts the mean sales value from the training set.
Model performance is then compared against this baseline.

If a machine learning model fails to outperform the baseline,
its practical usefulness is questionable.

"""

baseline_pred = np.full_like(y_test.values, y_train.mean(), dtype=float)
baseline_r2, baseline_mae, baseline_rmse = reg_metrics(y_test, baseline_pred)

baseline = pd.DataFrame([{
    "Model": "BaselineMean",
    "R2": baseline_r2,
    "MAE": baseline_mae,
    "RMSE": baseline_rmse
}])

pd.concat([baseline, results], ignore_index=True).sort_values("MAE")

"""## Cross-Validation

To reduce reliance on a single train–test split,
5-fold cross-validation is applied to all machine learning models.

This process:
- Trains models on multiple subsets of the data
- Evaluates performance across folds
- Provides more robust estimates of generalization performance

Average R², MAE, and RMSE across folds are used for model comparison.

"""

cv = KFold(n_splits=5, shuffle=True, random_state=42)

scoring = {
    "r2": "r2",
    "mae": "neg_mean_absolute_error",
    "rmse": "neg_root_mean_squared_error"
}

cv_rows = []
for name, model in models.items():
    out = cross_validate(model, X, y, cv=cv, scoring=scoring)
    cv_rows.append({
        "Model": name,
        "CV_R2_mean": out["test_r2"].mean(),
        "CV_MAE_mean": -out["test_mae"].mean(),
        "CV_RMSE_mean": -out["test_rmse"].mean()
    })

cv_df = pd.DataFrame(cv_rows).sort_values("CV_MAE_mean")
cv_df

# --- Feature sets
features_no_interaction = [
    "TV Ad Budget ($)",
    "Radio Ad Budget ($)",
    "Newspaper Ad Budget ($)",
    "Digital_Spend",
    "CTR",
    "Conversions",
    "ROI",
    "Engagement_Score"
]

features_with_interaction = features_no_interaction + ["TV_x_Digital"]

def make_pipeline(feats, model):
    prep = ColumnTransformer(
        transformers=[("num", StandardScaler(), feats)],
        remainder="drop"
    )
    return Pipeline([("prep", prep), ("model", model)])

cv = KFold(n_splits=5, shuffle=True, random_state=42)

scoring = {
    "r2": "r2",
    "mae": "neg_mean_absolute_error",
    "rmse": "neg_root_mean_squared_error"
}

comparison_rows = []

# Linear Regression
for label, feats in [
    ("Linear (No Interaction)", features_no_interaction),
    ("Linear (With Interaction)", features_with_interaction),
]:
    pipe = make_pipeline(feats, LinearRegression())
    scores = cross_validate(pipe, df[feats], y, cv=cv, scoring=scoring)
    comparison_rows.append({
        "Model": label,
        "CV_R2": scores["test_r2"].mean(),
        "CV_MAE": -scores["test_mae"].mean(),
        "CV_RMSE": -scores["test_rmse"].mean()
    })

# Random Forest
for label, feats in [
    ("RF (No Interaction)", features_no_interaction),
    ("RF (With Interaction)", features_with_interaction),
]:
    pipe = make_pipeline(
        feats,
        RandomForestRegressor(
            n_estimators=400, random_state=42, max_depth=12, min_samples_leaf=2
        )
    )
    scores = cross_validate(pipe, df[feats], y, cv=cv, scoring=scoring)
    comparison_rows.append({
        "Model": label,
        "CV_R2": scores["test_r2"].mean(),
        "CV_MAE": -scores["test_mae"].mean(),
        "CV_RMSE": -scores["test_rmse"].mean()
    })

interaction_comparison = pd.DataFrame(comparison_rows).sort_values("CV_MAE")
interaction_comparison

plt.figure(figsize=(8, 4))
plt.barh(
    interaction_comparison["Model"],
    interaction_comparison["CV_MAE"]
)
plt.xlabel("Cross-Validated MAE")
plt.title("Effect of Interaction Term (TV × Digital)")
plt.gca().invert_yaxis()
plt.tight_layout()
plt.show()

"""## Cross-Channel Synergy: Interaction vs No Interaction (H3)

To directly test the cross-channel synergy hypothesis (H3), models were estimated
with and without the interaction term between TV and Digital_Spend.

By keeping the learning algorithm constant and changing only the feature set,
this comparison isolates the contribution of the interaction effect.

Cross-validation results indicate whether including the interaction term improves
predictive performance (lower MAE / higher R²). An improvement supports the hypothesis
that traditional and digital channels reinforce each other rather than operating independently.

## Predicted vs Actual Sales

This visualization compares predicted sales values with actual sales values
for the best-performing model.

- Each point represents a test observation.
- The diagonal line represents perfect predictions.

The plot helps identify systematic bias, variance, and regions where
the model performs particularly well or poorly.
"""

best_name = results.iloc[0]["Model"]
best_pred = preds[best_name]

plt.figure(figsize=(6, 6))
plt.scatter(y_test, best_pred, alpha=0.7)
mn, mx = y_test.min(), y_test.max()
plt.plot([mn, mx], [mn, mx])
plt.xlabel("Actual Sales")
plt.ylabel("Predicted Sales")
plt.title(f"Predicted vs Actual — {best_name}")
plt.show()

"""## Linear Regression Coefficients

Linear regression coefficients represent the marginal effect of each feature on Sales,
holding other variables constant.

Because features are standardized, coefficients are directly comparable.
This allows interpretation of:
- Relative importance of marketing channels
- Direction of influence (positive or negative)

Coefficient analysis supports theory-driven insights into media effectiveness.
"""

coef = lin.named_steps["model"].coef_
coef_df = pd.DataFrame({"Feature": features, "Coefficient": coef}).sort_values("Coefficient", ascending=False)
coef_df

"""## Random Forest Feature Importance

Random Forest feature importance measures how much each variable contributes
to reducing prediction error across all decision trees.

Unlike linear coefficients, these importance scores capture:
- Nonlinear relationships
- Interaction effects
- Complex decision boundaries

Comparing feature importance with linear coefficients strengthens confidence
in consistently influential marketing channels.


"""

rf_model = rf.named_steps["model"]
fi = pd.DataFrame({"Feature": features, "Importance": rf_model.feature_importances_}).sort_values("Importance", ascending=False)
fi

libraries = """numpy
pandas
matplotlib
scikit-learn
"""

with open("requirements.txt", "w") as f:
    f.write(libraries)

print("requirements.txt")
